model:
    algo: AWSDM
    total_iter: 56000
    lr_steps: [32000, 48000]
    lr_mults: [0.1, 0.1]
    lr: 0.001
    weight_decay: 0.0001
    optim: SGD
    warmup_lr: []
    warmup_steps: []
    use_rgb: True
    use_std: True # newly added
    use_cnp: False # newly added
    backbone_arch: unet2sdm5skip
    backbone_param:
        in_channels: 1
        n_classes: 2
        use_deform: False # newly added
    inmask_weight: 5.
data:
    dataset: "COCOA"
    memcached: False
    memcached_client: "/mnt/lustre/share/memcached_client"
    trainval_dataset: 'AWDatasetSDM'
    train_image_root: "data/COCOA/train2014"
    train_annot_file: "data/COCOA/annotations/COCO_amodal_train2014.json"
    val_image_root: "data/COCOA/val2014"
    val_annot_file: "data/COCOA/annotations/COCO_amodal_val2014.json"

    input_size: 512
    enlarge_box: 3.
    eraser_front_prob: 0.8 # case 1 probability
    eraser_setter:
        min_overlap: 0.4
        max_overlap: 1.0
        min_cut_ratio: 0.001
        max_cut_ratio: 0.9
    base_aug:
        flip: True
        shift: [-0.2, 0.2]
        scale: [0.8, 1.2]
    load_rgb: True
    batch_size: 32
    batch_size_val: 32
    workers: 8
    angle: 10
    data_mean: [0.485, 0.456, 0.406]
    data_std: [0.229, 0.224, 0.225]
    use_default: False

trainer:
    initial_val: True
    val_freq: 200000
    val_iter: 10
    val_disp_start_iter: 0
    val_disp_end_iter: 1
    print_freq: 100
    save_freq: 2000
    loss_record: ['loss']
    tensorboard: True